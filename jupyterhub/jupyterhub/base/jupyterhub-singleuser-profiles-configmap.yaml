apiVersion: v1
kind: ConfigMap
metadata:
  name: jupyter-singleuser-profiles
data:
  jupyterhub-singleuser-profiles.yaml: |
      profiles:
      - name: globals
        env:
          S3_ENDPOINT_URL: $(s3_endpoint_url)
        resources:
          mem_limit: 2Gi
          cpu_limit: 1

      - name: Spark Notebook
        images:
        - 's2i-spark-minimal-notebook:3.6'
        - 's2i-spark-scipy-notebook:3.6'
        env:
          PYSPARK_SUBMIT_ARGS: '--conf spark.cores.max=6 --conf spark.executor.instances=2 --conf spark.executor.memory=3G --conf spark.executor.cores=3 --conf spark.driver.memory=4G --packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell'
          PYSPARK_DRIVER_PYTHON: 'jupyter'
          PYSPARK_DRIVER_PYTHON_OPTS: 'notebook'
          SPARK_HOME: '/opt/app-root/lib/python3.6/site-packages/pyspark/'
          PYTHONPATH: '$PYTHONPATH:/opt/app-root/lib/python3.6/site-packages/:/opt/app-root/lib/python3.6/site-packages/pyspark/python/:/opt/app-root/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.8.2.1-src.zip'
        services:
          spark:
            template: 'jupyterhub-spark-operator-configmap'
            parameters:
              WORKER_NODES: '2'
              MASTER_NODES: '1'
              MASTER_MEMORY_LIMIT: '2Gi'
              MASTER_CPU_LIMIT: '1'
              MASTER_MEMORY_REQUEST: '2Gi'
              MASTER_CPU_REQUEST: '1'
              WORKER_MEMORY_LIMIT: '2Gi'
              WORKER_CPU_LIMIT: '1'
              WORKER_MEMORY_REQUEST: '2Gi'
              WORKER_CPU_REQUEST: '1'
              SPARK_IMAGE: 'quay.io/opendatahub/spark-cluster-image:spark22python36'
            return:
              SPARK_CLUSTER: 'metadata.name'

      sizes:
      - name: Small
        resources:
          mem_limit: 2Gi
          cpu_limit: 2
      - name: Medium
        resources:
          mem_limit: 4Gi
          cpu_limit: 4
      - name: Large
        resources:
          mem_limit: 8Gi
          cpu_limit: 8
